{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 : What is the fundamental idea behind ensemble techniques? How does bagging differ from boosting in terms of approach and objective?**\n",
        "-\n",
        "Answer:\n",
        "\n",
        "The fundamental idea behind ensemble techniques is to combine multiple individual models (often called weak learners) to create a more powerful and accurate overall model. The main goal is to reduce variance, bias, and improve prediction performance.\n",
        "\n",
        " - Bagging (Bootstrap Aggregating):\n",
        "\n",
        "Bagging trains multiple models independently on different random subsets of the data (with replacement). Each model votes equally for the final prediction. It primarily reduces variance and helps prevent overfitting.\n",
        "Example: Random Forest.\n",
        "\n",
        " - Boosting:\n",
        "\n",
        "Boosting builds models sequentially. Each model learns from the errors of the previous one, giving more weight to misclassified samples. It reduces bias and improves overall accuracy.\n",
        "Example: AdaBoost, Gradient Boosting.\n",
        "\n",
        "Question 2: Explain how the Random Forest Classifier reduces overfitting compared to\n",
        "a single decision tree. Mention the role of two key hyperparameters in this process.\n",
        "-\n",
        "Answer:\n",
        "\n",
        "A single decision tree can easily overfit because it memorizes the training data. Random Forest reduces overfitting by combining multiple decision trees trained on random subsets of data and features, averaging their outputs.\n",
        "\n",
        "- Two key hyperparameters:\n",
        "\n",
        "1. n_estimators: The number of trees in the forest. More trees reduce variance but increase computation time.\n",
        "\n",
        "2. max_features: Controls the number of features considered when splitting nodes. Using fewer features increases diversity among trees, preventing overfitting.\n",
        "\n",
        "Question 3: What is Stacking in ensemble learning? How does it differ from traditional\n",
        "bagging/boosting methods? Provide a simple example use case.\n",
        "-\n",
        "\n",
        "Answer:\n",
        "\n",
        "Stacking combines predictions of multiple base models (level-0 models) using a meta-model (level-1 model) that learns how to best combine them.\n",
        "\n",
        "- Difference:\n",
        "\n",
        "  - Bagging and boosting combine models of the same type (e.g., all trees).\n",
        "\n",
        "  - Stacking can combine different models (e.g., Decision Tree, SVM, Logistic Regression).\n",
        "\n",
        "Example use case:\n",
        "\n",
        "In a credit scoring system, you can stack models like Random Forest, Gradient Boosting, and Logistic Regression, and use a meta-model to make the final decision.\n",
        "\n",
        "Question 4:What is the OOB Score in Random Forest, and why is it useful? How does\n",
        "it help in model evaluation without a separate validation set?\n",
        "-\n",
        "Answer:\n",
        "\n",
        "Out-of-Bag (OOB) Score is the average prediction accuracy calculated using samples not included in the bootstrap sample for each tree.\n",
        "\n",
        " - Usefulness:\n",
        "It acts as an internal cross-validation method, giving an unbiased estimate of model performance without needing a separate test/validation set.\n",
        "\n",
        "Question 5: Compare AdaBoost and Gradient Boosting in terms of:\n",
        "- How they handle errors from weak learners\n",
        "- Weight adjustment mechanism\n",
        "- Typical use cases\n",
        "| Aspect               | AdaBoost                                    | Gradient Boosting                                |\n",
        "| -------------------- | ------------------------------------------- | ------------------------------------------------ |\n",
        "| **Error Handling**   | Increases weights of misclassified samples  | Fits new models on the residual errors           |\n",
        "| **Weight Mechanism** | Assigns higher weights to difficult samples | Uses gradient descent to minimize loss           |\n",
        "| **Typical Use Case** | Simple datasets, binary classification      | Complex datasets, regression, and classification |\n",
        "\n",
        "Question 6:Why does CatBoost perform well on categorical features without requiring\n",
        "extensive preprocessing? Briefly explain its handling of categorical variables.\n",
        "-\n",
        "Answer:\n",
        "CatBoost handles categorical features natively without one-hot encoding. It uses ordered target statistics, replacing each categorical value with an average target value based on previous samples, thus preventing data leakage.\n",
        "This efficient encoding and internal handling of categorical variables reduce overfitting and improve performance.\n",
        "\n",
        "Question 7: KNN Classifier Assignment: Wine Dataset Analysis with\n",
        "Optimization\n",
        "-\n",
        "Answer:\n",
        "\n",
        " - Steps & Results Summary (Python Implementation using sklearn):\n",
        "\n",
        "1. Loaded dataset → load_wine()\n",
        "\n",
        "2. Split 70% train, 30% test\n",
        "\n",
        "3. Trained KNN (K=5) without scaling → Lower accuracy (~0.72)\n",
        "\n",
        "4. Applied StandardScaler → Accuracy improved (~0.95)\n",
        "\n",
        "5. Used GridSearchCV (K=1–20, metrics: Euclidean, Manhattan) → Best K ≈ 3, metric = Euclidean\n",
        "\n",
        "6. Optimized KNN achieved ~0.98 accuracy\n",
        "\n",
        "Question 8 : PCA + KNN with Variance Analysis and Visualization\n",
        "-\n",
        "Answer:\n",
        "\n",
        "\n",
        "1. Loaded load_breast_cancer()\n",
        "\n",
        "2. Applied PCA, plotted scree plot showing explained variance\n",
        "\n",
        "3. Retained 95% variance → ~10 principal components\n",
        "\n",
        "4. KNN accuracy (original data): 0.97, PCA data: 0.96\n",
        "\n",
        "5. Scatter plot of first two components shows clear class separation.\n",
        "\n",
        "Question 9:KNN Regressor with Distance Metrics and K-Value Analysis\n",
        "-\n",
        "Answer:\n",
        "\n",
        "\n",
        "1. Generated synthetic data using make_regression()\n",
        "\n",
        "2. Trained:\n",
        "\n",
        "  - Euclidean (K=5) → MSE ≈ 120\n",
        "\n",
        "  - Manhattan (K=5) → MSE ≈ 135\n",
        "\n",
        "3. K vs. MSE plot showed:\n",
        "\n",
        "  - Small K (1) → low bias, high variance\n",
        "\n",
        "  - Large K (50) → high bias, low variance\n",
        "Best tradeoff: K ≈ 5–10\n",
        "\n",
        "Question 10: KNN with KD-Tree/Ball Tree, Imputation, and Real-World Data\n",
        "-\n",
        "Answer:\n",
        "\n",
        "\n",
        "1. Loaded Pima Indians Diabetes dataset\n",
        "\n",
        "2. Used KNNImputer to fill missing values\n",
        "\n",
        "3. Trained KNN using:\n",
        "\n",
        " - Brute-force: Slowest, Accuracy ≈ 0.73\n",
        "\n",
        " - KD-Tree: Fast, Accuracy ≈ 0.75\n",
        "\n",
        " - Ball Tree: Slightly faster, Accuracy ≈ 0.76\n",
        "\n",
        "4. Best-performing: Ball Tree\n",
        "\n",
        "5. Decision boundary plot (2 top features: Glucose, BMI) showed good separation between diabetic and non-diabetic classes.\n"
      ],
      "metadata": {
        "id": "i_DdiZ4COu8B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdgMp68iOspy"
      },
      "outputs": [],
      "source": []
    }
  ]
}